{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**\n",
    "\n",
    "Instacart is a grocery ordering and delivery app that aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.\n",
    "\n",
    "Instacart has:\n",
    "- 100s of retailers\n",
    "- 10,000s of stores\n",
    "- 10,000s of shoppers\n",
    "- 1,00,000s of products\n",
    "- 100,000,000s of items\n",
    "\n",
    "Instacart's data science team uses transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session.\n",
    "\n",
    "(https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe)\n",
    "\n",
    "(https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)\n",
    "\n",
    "**Data**\n",
    "\n",
    "The dataset is a relational set of files describing customers' orders over time. The goal is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200K Instacart users. For each user, we provide between 4 and 100 of their orders, with the sequence of products purchased in each order. We also provide the week and hour of day the order was placed, and a relative measure of time between orders.\n",
    "\n",
    "(https://www.kaggle.com/c/instacart-market-basket-analysis)\n",
    "\n",
    "**Possible problem**\n",
    "\n",
    "Because Instacart does not have the logistics supply chain information for products, when an item that a customer adds to the cart is unavailable in store, it costs every stakeholder in Instacart's marketplace. Shoppers waste time searching for an unavailable item, customers can't buy what they want, and retail partners lose out on revenue.\n",
    "\n",
    "By proactively and accurately predicting customers' buying behavior, Instacart can use this information to match and search through their availability prediction model of whether a certain item out of the 200 million grocery items is available in real-time and make appropriate recommendations to the customer for the out-of-stock item(s) if applicable.\n",
    "\n",
    "**Possible prediction tasks**\n",
    "- Products that a user will buy again\n",
    "- Products that a user will try for the first time\n",
    "- Products that a user will add to cart next during a session\n",
    "- Products that a user will buy together\n",
    "- Time that a user will make the next purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's load in the `mba` and `num_items_order` DataFrames from the \"instacart_market_basket_analysis\" notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mba = pd.read_pickle('mba.pickle')\n",
    "num_items_order = pd.read_pickle('num_items_order.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Basket Analysis\n",
    "\n",
    "**Association analysis: what is the probability that customers will buy product A with product B?**\n",
    "\n",
    "(https://www.kaggle.com/datatheque/association-rules-mining-market-basket-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Association analysis**: \n",
    "\n",
    "- Association rules are written like {**`antecedent`**} -> {**`consequent`**}\n",
    "- Both `antecedents` and `consequents` can have multiple items\n",
    "\n",
    "\n",
    "- **`Support`** is the percentage of transactions that contain all the items in an itemset\n",
    "    - High support used to make sure there is a useful relationship\n",
    "    - Low support used to find \"hidden\" relationships\n",
    "    \n",
    "    \n",
    "- **`Confidence`** is the probability that a transaction that contains the items in the `antecedent` also contains the items in the `consequent`\n",
    "    - **confidence{A -> B} = support{A, B}/support{A}**\n",
    "    - `Confidence` of 0.5 means that in 50% of the cases where the `antecedent` was purchased, the purchase also included the `consequent`\n",
    "    - The higher the `confidence` the greater the likelihood that the `consequents` will be purchased with the `antecedents`\n",
    "    \n",
    "    \n",
    "- **`Lift`** is the probability of all the items in a rule occurring together divided by the product of the probabilities of the `antecedents` and `consequents` occuring as if they were independent of each other\n",
    "    - `Lift` indicates whether there is a relationship between A and B or whether the two items are occuring in the same orders simply by chance\n",
    "    - `Lift` = 1 implies no relationship (co-occur at random) between A and B\n",
    "    - `Lift` > 1 implies that there is a positive relationship (co-occur more often than random) between A and B\n",
    "    - `Lift` < 1 implies that there is a negative relationship (co-occur less often than random) between A and B \n",
    "    - **lift{A, B} = lift{B, A}** unlike for `confidence`\n",
    "    \n",
    "(http://pbpython.com/market-basket-analysis.html)\n",
    "\n",
    "**Apriori algorithm**:\n",
    "\n",
    "- Apriori algorithm, a data mining algorithm, is used to perfom a market basket analysis and identify potential rules\n",
    "    1. Set a minimum value for support and confidence\n",
    "    2. Extract all the subsets having higher value of support than the minimum threshold\n",
    "    3. Select all the rules from the subsets with confidence value higher than minimum threshold\n",
    "    4. Order the rules by descending order of `lift`\n",
    "    - Apriori algorithm does not use `lift` to establish rules, but `lift` is used when exploring the rules the algorithm returns\n",
    "\n",
    "(https://select-statistics.co.uk/blog/market-basket-analysis-understanding-customer-behaviour/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: MLxtend\n",
    "\n",
    "(http://pbpython.com/market-basket-analysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll be using MLxtend library by Sebastian Raschka because scikit-learn has no built in Apriori algorithm for extracting frequent item sets. `pip install mlxtend` did not work for me so I used `conda install -c rasbt mlxtend` instead which worked! When I had mlxtend installed, it said the \"frequent_patterns\" modul couldn't be found, so I used `pip install git+git://github.com/rasbt/mlxtend.git` to resolve the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions with 10 or less items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because the `mba` dataset contains so many transactions and unique products, let's look at a subset of the data -- transactions with 2-10 items, which is about 60% of the orders.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartsize_10 = num_items_order[(num_items_order['num_of_items']<=10) & (num_items_order['num_of_items']>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mba_10 = mba.merge(cartsize_10, how='right', on='order_id')\n",
    "mba_subset =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "mba_subset = dd.from_pandas(mba_subset, npartitions=1+mba_subset.memory_usage().sum()//1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to first convert our dataframe into the format where each row represents a cart and each column is a unique product name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "mba_basket = mba_subset.groupby(['order_id', 'product_name'])['organic'].count().reset_index().compute()\n",
    "mba_basket.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time\n",
    "mba_basket = mba_basket.unstack(fill_value=0).compute()\n",
    "mba_basket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because the `apriori` fuction takes one-hot encoded dataframes and we want to know whether a product was present in a cart or not, we will one-hot encode the counts as 0 (did not purchase) and 1 (purchased).**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return(0)\n",
    "    if x >=1:\n",
    "        return(1)\n",
    "    \n",
    "org_basket_sets = organic_basket.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With our data one-hot encoded, we can generate frequent item sets. But first we need to test and select a `support` threshold, which is fairly small because there are so many distinct items and combinations of items in a cart at a time.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "frequent_itemsets = apriori(basket_sets, min_support=0.05, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's generate our association rules based on the frequent item sets created above with their corresponding `support`, `confidence`, and `lift`.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rules = association_rules(frequent_itemsets, metrics='lift', min_threshold=1)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rules with high `lift` values indicates that it occurs more frequently than would be expected given the number of transaction and product combinations. Let's filter for rules with fairly high `confidence` and `lift`.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rules[(rules['lift']>=) & (rules['confidence']>=)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that and are purchased together in a manner that is higher than the overall probability would suggest. Let's look at how much opportunity there is to use the popularity of one product to drive the sales of another.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(organic_basket[''].sum())\n",
    "print(organic_basket[''].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: MLxtend Transaction Encoder\n",
    "\n",
    "(http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_num = mba_basket.reset_index().order_id.nunique()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "print('There are {} orders with 2-10 items.'.format(ord_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_ids = mba_basket.reset_index().order_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because the dataset is so large, our kernel keeps crashing, so let's save the `list_of_lists` so we don't have to run the above code and create it every time after the kernel crashes.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time\n",
    "list_of_lists = []\n",
    "for i in range(ord_num):\n",
    "    itemsets = mba_basket.loc[mba_basket['order_id'] == ord_ids[i], 'product_name'].tolist()\n",
    "    list_of_lists.append(itemsets)\n",
    "list_of_lists[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('list_of_items.pickle', 'wb') as f:\n",
    "       pickle.dump(list_of_lists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_items.pickle', 'rb') as f:\n",
    "       list_of_lists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransactionEncoder() takes in a list of list of products in each transaction and converts it into a format where each column is a unqiue item. If the dataset contains small transactions but lots of unique products, representing the data in sparse format can save memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(list_of_lists)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "df = dd.from_pandas(df, npartitions=1+df.memory_usage().sum()//1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's return the items and itemsets with at least 10% support.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's filter through the results for itemsets of size 2 or more that has a support of at least 40%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets['size'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[(frequent_itemsets['size'] == 2) & (frequent_itemsets['support'] >= 0.4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Apyori\n",
    "\n",
    "(https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apyori as apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the Apyori library requires that the dataset to be in the form of a list of lists where the whole dataset is a big list and each transaction in the dataset is an inner list within the outer big list, we need to convert our dataframe into a list of lists, which we have done in method 2 already.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The apriori class from the apyori library has several parameters, such as for the support threshold, the confidence threshold, the lift threshold, and the minimum number of items included in the rules, that can be altered.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = apriori(list_of_lists, min_support=0.05, min_confidence=0.2, min_lift=3, min_length=2)\n",
    "association_results = list(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} association rules mined by the Apriori algorithm.'.format(len(associaton_rules)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in association_rules:\n",
    "    pair = item[0]\n",
    "    items = [x for x in pair]\n",
    "    print('Rule: ' + items[0] + '->' + items[1])\n",
    "    print('Support: ' + str(item[1]))\n",
    "    print('Confidence: ' + str(item[2][0][2]))\n",
    "    print('Lift: ' + str(item[2][0][3]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion/Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the analysis reflects how frequently items co-occur in transactions. This is a function both of the strength of association between the items, and the way the site owner has presented them.\n",
    "\n",
    "To say that in a different way: items might cooccur not because they are “naturally” connected, but because we, the people in charge of the site, have presented them together.\n",
    "\n",
    "This is an example of a more general problem in web analytics: our data reflects the way users behave, and the way we have encouraged them to behave, by the website design decisions we have made.\n",
    "\n",
    "There are a number of ways we can use the data to drive site organisation:\n",
    "\n",
    "- Large clusters of co-occuring items should probably be placed in their own category / theme\n",
    "- Item pairs that commonly co-occur should be placed close together within broader categories on the website. This is especially important where one item in a pair is very popular, and the other item is very high margin.\n",
    "- Long lists of rules (including ones with low support and confidence) can be used to put recommendations at the bottom of product pages and on product cart pages. The only thing that matters for these rules is that the lift is greater than one. (And that we pick those rules that are applicable for each product with the high lift where the product recommended has a high margin.)\n",
    "- In the event that doing the above (3) drives significant uplift in profit, it would strengthen the case to invest in a recommendation system, that uses a similar algorithm in an operational context to power automatic recommendation engine on your website.\n",
    "\n",
    "Using the data for targeted marketing\n",
    "- The same results can be used to drive targeted marketing campaigns. For each user, we pick a handful of products based on products they have bought to date which have both a high uplift and a high margin, and send them a e.g. personalized email or display ads etc.\n",
    "- How we use the analysis has significant implications for the analysis itself: if we are feeding the analysis into a machine-driven process for delivering recommendations, we are much more interested in generating an expansive set of rules. If, however, we are experimenting with targeted marketing for the first time, it makes much more sense to pick a handful of particularly high value rules, and action just them, before working out whether to invest in the effort of building out that capability to manage a much wider and more complicated rule set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
