{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Search Scraper\n",
    "\n",
    "Job search sites such as Glassdoor and Indeed are very useful with their filters and job alerts, but these sites don't have an advanced filter for education level and years of experience. For example, the \"entry level\" filter often displays results for senior positions or asks for 5+ years of experience. Therefore, scanning each job description and narrowing down the search results that fit my criteria manually is very time consuming, hence the spark of the idea for this project!\n",
    "\n",
    "Because of the aforementioned restrictions, I want to personalize and automate the job search by scraping sites for job postings, narrowing down results based on my criteria, and updating a CSV file of relevant postings.\n",
    "\n",
    "(https://github.com/umangkshah/job-scraping-python/blob/master/job_scraper.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Logic\n",
    "\n",
    "1. First, before scraping anything, it is a good practice to check if there is an API to fetch the data. If there isn't, web scraping is the alternate option. Web scraping can also be used when the API is not retrieving the information that we want.\n",
    "    - API\n",
    "        - Advantanges:\n",
    "            - Much more stable process for retrieving info\n",
    "            - Extremely regulated syntax (JSON or XML rather than HTML)\n",
    "        - Disadvantages:\n",
    "            - Query limitations\n",
    "            - Less customizable because governed by API regulations\n",
    "            - API can disappear\n",
    "    - Webscraping\n",
    "        - Advantanges:\n",
    "            - Inexpensive\n",
    "            - Easy to implement\n",
    "            - Low maintance\n",
    "            - Accurate\n",
    "        - Disadvantages:\n",
    "            - Less stable because uses HTML/CSS fields to capture data\n",
    "            - Will crash if front end labels are changed\n",
    "            - Slower than API calls\n",
    "\n",
    "If API is not available:\n",
    "\n",
    "2. Construct the URL for the search results from the job search sites (Indeed, LinkedIn, Glassdoor).\n",
    "2. Download HTML of the search result page using Python Requests\n",
    "3. Parse the page using LXML: LXML lets you navigate the HTML Tree Structure using Xpaths \n",
    "4. Save the data to a CSV file\n",
    "\n",
    "(https://www.scrapehero.com/how-to-scrape-job-listings-from-glassdoor-using-python-and-lxml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Indeed to Retrieve Job Search Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html, etree\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import unicodecsv as csv\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib as ul\n",
    "import requests\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalize the Filters\n",
    "\n",
    "**Let's create a list of words to avoid or include.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_flags = ['senior', 'sr.', 'staff', 'manager', 'director', 'lead', 'head', 'principal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's write a function that determines whether or not to check a job posting based on whether the title contains red flag words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def title_qualifies(title):\n",
    "    title = title.lower()\n",
    "    for word in red_flags:\n",
    "        if word in title:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "title_qualifies('Director of Data Science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, let's define the Regex to personalize filters for:**\n",
    "1. **Years of experience: no more than two years of experience required**\n",
    "2. **Education level: Bachelor or BA or BS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<_sre.SRE_Match object at 0x10bbefd30>\n",
      "<_sre.SRE_Match object at 0x10bbefd30>\n",
      "None\n",
      "<_sre.SRE_Match object at 0x10bbefd30>\n",
      "<_sre.SRE_Match object at 0x10bbefd30>\n",
      "<_sre.SRE_Match object at 0x10bbefd30>\n",
      "<_sre.SRE_Match object at 0x10bbefd30>\n"
     ]
    }
   ],
   "source": [
    "# Should not have 3 or more years of experience\n",
    "yr_exper = re.compile('[3-9]\\s*\\+?-?\\s*[2-9]?\\s*[Yy]e?a?[Rr][Ss]?')\n",
    "\n",
    "# Should not have master's requirement\n",
    "masters1 = re.compile(\"[Mm]aster's required\")\n",
    "masters2 = re.compile('[Ms][Ss] required')\n",
    "masters3 = re.compile('[Mm].[Ss]. required')\n",
    "masters4 = re.compile('[Mm][Bb][Aa] required')\n",
    "masters5 = re.compile('[Mm].[Bb].[Aa]. required')\n",
    "\n",
    "# Should not have PHD requirement\n",
    "phd1 = re.compile('[Pp][Hh][Dd] required')\n",
    "phd2 = re.compile('[Pp][Hh].[Dd]. required')\n",
    "phd3 = re.compile('[Pp].[Hh].[Dd]. required')\n",
    "\n",
    "# Should not have any advanced degree requirement\n",
    "adv_deg = re.compile('[Aa]dvanced degree required')\n",
    "\n",
    "print(yr_exper.search('2 years of experience'))\n",
    "print(yr_exper.search('2+ years of experience'))\n",
    "print(yr_exper.search('2-4 years of experience'))\n",
    "print(masters1.search(\"master's required\"))\n",
    "print(masters1.search('bachelor'))\n",
    "print(masters3.search('M.s. required'))\n",
    "print(phd1.search('PHd required'))\n",
    "print(phd2.search('PH.d. required'))\n",
    "print(adv_deg.search('Advanced degree required'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When looking in the url bar in the browser, we can extract the base url. Usually, the page number is formatted at the end of the url to indicate which page of the search results you are on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ready.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed_base_url = 'https://www.indeed.com/jobs?q=data+(science+or+analysis+or+python+or+machine+or+learning+or+statistics)+-senior,+-manager,+-staff,+-head,+-director,+-sr.,+-principal,+-lead,+-JAVA,+-CSS,+-C%2B%2B,+-C%2B,+-HTML,+-full+-stack,+-front+-end,+-back+-end&l=San+Francisco+Bay+Area,+CA&limit=50&start'\n",
    "#indeed_base_url = 'https://www.indeed.com/jobs?q=data+(science+or+analysis+or+python+or+machine+or+learning+or+statistics)+-senior,+-manager,+-staff,+-head,+-director,+-sr.,+-principal,+-lead,+-JAVA,+-CSS,+-C%2B%2B,+-C%2B,+-HTML,+-full+-stack,+-front+-end,+-back+-end&l=San+Francisco+Bay+Area,+CA&limit=50&radius=25&start='\n",
    "pg_num = 0\n",
    "\n",
    "try:\n",
    "    response = ul.urlopen(indeed_base_url + str(pg_num))\n",
    "    html_doc = response.read()\n",
    "except:\n",
    "    print('URL not accessible')\n",
    "    \n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "'Ready.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see how many jobs have returned from our search query on Indeed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1474 jobs on Indeed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for d in soup.select('div'):\n",
    "        # Get the text which is in the format \"Page 1 of {total_results} jobs\"\n",
    "        total_results = soup.find(id='searchCount').get_text()\n",
    "        # Strip all the text in from of the total_results number\n",
    "        total_results = total_results[total_results.find('of ') + 3:]\n",
    "        # Strip all the text after the total_results number\n",
    "        total_results = total_results[:total_results.find(' jobs')]\n",
    "        # Remove the comma and convert the string into an integer\n",
    "        total_results = int(total_results.replace(',', ''))\n",
    "    print('There are {} jobs on Indeed.'.format(total_results))\n",
    "except:\n",
    "    print('No jobs found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create a function building this a webscraper from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indeed(url_base, jobs_per_page):\n",
    "    \"\"\"\n",
    "    Write a function that scrapes Indeed for the first time. After the first time, we will automate the web scraper to\n",
    "    scrape new jobs every day and append the results to the existing csv file.\n",
    "    \"\"\"    \n",
    "    # Use the first page of the search results to create a BeautifulSoup object\n",
    "    pg_num = 0\n",
    "    response = ul.urlopen(url_base + str(pg_num))\n",
    "    html_doc = response.read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    # Print the number of jobs returned from the search query\n",
    "    for d in soup.select('div'):\n",
    "        total_results = soup.find(id='searchCount').get_text()\n",
    "        total_results = total_results[total_results.find('of ') + 3:]\n",
    "        total_results = total_results[:total_results.find(' jobs')]\n",
    "        total_results = int(total_results.replace(',', ''))\n",
    "    print('There are {} jobs on Indeed.'.format(total_results))\n",
    "    \n",
    "    # Extract the desired job posting information for the jobs that meet our filtering criteria\n",
    "    last_page = (total_results + jobs_per_page - 1) // jobs_per_page\n",
    "    job_listings = []\n",
    "    print('Getting jobs from Indeed...')\n",
    "    for num in range(0, last_page * jobs_per_page, jobs_per_page):\n",
    "        try:\n",
    "            response = ul.urlopen(url_base + str(num))\n",
    "            html = response.read()\n",
    "        except:\n",
    "            break;\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for post in soup.find_all(class_='result'):\n",
    "            # Get job post URL\n",
    "            link = post.find(class_='turnstileLink')\n",
    "            # Get job title aka jt\n",
    "            try:\n",
    "                jt = link.get('title')\n",
    "            except:\n",
    "                jt = ''\n",
    "            # Get job post company\n",
    "            try:\n",
    "                comp = post.find(class_='company').get_text().strip()\n",
    "            except:\n",
    "                comp = ''\n",
    "            # Get job post location\n",
    "            try:\n",
    "                location = post.find(class_='location').get_text().strip()\n",
    "            except:\n",
    "                location = ''\n",
    "            # Get job post salary\n",
    "            try:\n",
    "                salary = post.find(class_='salary no-wrap').get_text().strip()\n",
    "            except:\n",
    "                salary = ''\n",
    "\n",
    "            if(title_qualifies(jt)):\n",
    "                job_match_url = 'http://www.indeed.com' + link.get('href')\n",
    "                try:\n",
    "                    html_doc = ul.urlopen(job_match_url).read().decode('utf-8')\n",
    "                except:\n",
    "                    continue;\n",
    "\n",
    "                a = yr_exper.search(html_doc)\n",
    "                b = masters1.search(html_doc)\n",
    "                c = masters2.search(html_doc)\n",
    "                d = masters3.search(html_doc)\n",
    "                e = masters4.search(html_doc)\n",
    "                f = masters5.search(html_doc)\n",
    "                g = phd1.search(html_doc)\n",
    "                h = phd2.search(html_doc)\n",
    "                i = phd3.search(html_doc)\n",
    "                j = adv_deg.search(html_doc)\n",
    "                if not any([a, b, c, d, e, f, g, h, i, j]):\n",
    "                    jobs = {\n",
    "                            'Job Title': jt,\n",
    "                            'Company': comp,\n",
    "                            'Location': location,\n",
    "                            'Salary': salary,\n",
    "                            'URL': job_match_url\n",
    "                            }\n",
    "                    job_listings.append(jobs)\n",
    "    \n",
    "    # Export as a csv file\n",
    "    with open('indeed_job_results.csv', 'wb') as csvfile:\n",
    "        fieldnames = ['Job Title', 'Company', 'Location', 'Salary', 'URL']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        if job_listings:\n",
    "            for job in job_listings:\n",
    "                writer.writerow(job)\n",
    "            print('Done!')\n",
    "        else:\n",
    "            print('No matches for Data Science jobs in Indeed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1474 jobs on Indeed.\n",
      "Getting jobs from Indeed...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scrape_indeed(indeed_base_url, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the Web Scraper to Scrape New Job Postings in the Last 3 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indeed_recent_base_url = 'https://www.indeed.com/jobs?q=data+%28science+or+analysis+or+python+or+machine+or+learning+or+statistics%29+-senior%2C+-manager%2C+-staff%2C+-head%2C+-director%2C+-sr.%2C+-principal%2C+-lead%2C+-JAVA%2C+-CSS%2C+-C%2B%2B%2C+-C%2B%2C+-HTML%2C+-full+-stack%2C+-front+-end%2C+-back+-end&l=San+Francisco+Bay+Area%2C+CA&sort=date&limit=50&fromage=3&radius=25&start='\n",
    "#scrape_indeed(indeed_recent_base_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
