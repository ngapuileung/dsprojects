{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Search Scraper\n",
    "\n",
    "Job search sites such as Glassdoor and Indeed are very useful with their filters and job alerts, but these sites don't have an advanced filter for education level and years of experience. For example, the \"entry level\" filter often displays results for senior positions or asks for 5+ years of experience. Therefore, scanning each job description and narrowing down the search results that fit my criteria manually is very time consuming, hence the spark of the idea for this project!\n",
    "\n",
    "Because of the aforementioned restrictions, I want to personalize and automate the job search by scraping sites for job postings, narrowing down results based on my criteria, and updating a CSV file of relevant postings.\n",
    "\n",
    "(https://github.com/umangkshah/job-scraping-python/blob/master/job_scraper.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Logic\n",
    "\n",
    "1. First, before scraping anything, it is a good practice to check if there is an API to fetch the data. If there isn't, web scraping is the alternate option. Web scraping can also be used when the API is not retrieving the information that we want.\n",
    "    - API\n",
    "        - Advantanges:\n",
    "            - Much more stable process for retrieving info\n",
    "            - Extremely regulated syntax (JSON or XML rather than HTML)\n",
    "        - Disadvantages:\n",
    "            - Query limitations\n",
    "            - Less customizable because governed by API regulations\n",
    "            - API can disappear\n",
    "    - Webscraping\n",
    "        - Advantanges:\n",
    "            - Inexpensive\n",
    "            - Easy to implement\n",
    "            - Low maintance\n",
    "            - Accurate\n",
    "        - Disadvantages:\n",
    "            - Less stable because uses HTML/CSS fields to capture data\n",
    "            - Will crash if front end labels are changed\n",
    "            - Slower than API calls\n",
    "\n",
    "If API is not available or manual scraping is needed:\n",
    "\n",
    "1. Construct the URL for the search results from the job search sites (Indeed, LinkedIn, Glassdoor).\n",
    "2. For Glassdoor, we'll be using Selenium because it allows us to browse through a website mimicking the behavior of Chrome and insures that our code receives the content that we see in the browser even when content is purposely hidden from scraping.\n",
    "3. Record a brief description of the post and relative link.\n",
    "4. Browse through the list of links and retrieve the post description.\n",
    "5. Both steps 3 and 4 are achieved using a self-defined `glassdoorScrape()` function and `get_short=True` parameter for short descriptions and `get_short=False` parameter for the complete post.\n",
    "\n",
    "(https://github.com/nycdatasci/bootcamp006_project/blob/master/Project3-WebScraping/DiegoDeLazzari/Presentation2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Indeed to Retrieve Job Search Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip3 install selenium\n",
    "!pip3 install parsel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common import action_chains, keys, alert\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib as ul\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalize the Filters\n",
    "\n",
    "**Let's create a list of words to avoid or include.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red_flags = ['senior', 'sr.', 'staff', 'manager', 'director', 'lead', 'head', 'principal', 'photography', 'journalist',\n",
    "             'frontend', 'backend', 'fullstack', 'front-end', 'back-end', 'full-stack', 'front end', 'back end', \n",
    "             'full stack', 'bio', 'recruiter', 'sourcer', 'phd', 'master', 'mba', 'lecturer', 'tutor', 'instructor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's write a function that determines whether or not to check a job posting based on whether the title contains red flag words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def title_qualifies(title):\n",
    "    title = title.lower()\n",
    "    for word in red_flags:\n",
    "        if word in title:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "title_qualifies('Director of Data Science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, let's define the Regex to personalize filters for:**\n",
    "1. **Years of experience: no more than two years of experience required**\n",
    "2. **Education level: Bachelor or BA or BS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<_sre.SRE_Match object at 0x1a155b3ac0>\n",
      "<_sre.SRE_Match object at 0x1a155b3ac0>\n",
      "None\n",
      "<_sre.SRE_Match object at 0x1a155b3ac0>\n",
      "<_sre.SRE_Match object at 0x1a155b3ac0>\n",
      "<_sre.SRE_Match object at 0x1a155b3ac0>\n",
      "<_sre.SRE_Match object at 0x1a155b3ac0>\n"
     ]
    }
   ],
   "source": [
    "# Should not have 3 or more years of experience\n",
    "yr_exper = re.compile('[3-9]\\s*\\+?-?\\s*[2-9]?\\s*[Yy]e?a?[Rr][Ss]?')\n",
    "\n",
    "# Should not have master's requirement\n",
    "masters1 = re.compile(\"[Mm]aster's required\")\n",
    "masters2 = re.compile('[Ms][Ss] required')\n",
    "masters3 = re.compile('[Mm].[Ss]. required')\n",
    "masters4 = re.compile('[Mm][Bb][Aa] required')\n",
    "masters5 = re.compile('[Mm].[Bb].[Aa]. required')\n",
    "\n",
    "# Should not have PHD requirement\n",
    "phd1 = re.compile('[Pp][Hh][Dd] required')\n",
    "phd2 = re.compile('[Pp][Hh].[Dd]. required')\n",
    "phd3 = re.compile('[Pp].[Hh].[Dd]. required')\n",
    "\n",
    "# Should not have any advanced degree requirement\n",
    "adv_deg = re.compile('[Aa]dvanced degree required')\n",
    "\n",
    "print(yr_exper.search('2 years of experience'))\n",
    "print(yr_exper.search('2+ years of experience'))\n",
    "print(yr_exper.search('2-4 years of experience'))\n",
    "print(masters1.search(\"master's required\"))\n",
    "print(masters1.search('bachelor'))\n",
    "print(masters3.search('M.s. required'))\n",
    "print(phd1.search('PHd required'))\n",
    "print(phd2.search('PH.d. required'))\n",
    "print(adv_deg.search('Advanced degree required'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code below tests how to cross out popup alerts in Glassdoor. As of now I still haven't cracked this problem, so I need to manually click the \"X\" button for two popups for my code to fully run on the 30 pages of search results in Glassdoor. It's limited to 30 pages because Glassdoor can't retrieve jobs after the 30th page so roughly after 900 jobs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python2.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: use options instead of chrome_options\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darn! Couldn't find it :(\n"
     ]
    }
   ],
   "source": [
    "path_name = '/Users/ngapuileung/Desktop/DS/chromedriver'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--disable-extensions')\n",
    "chrome_options.add_argument('--profile-directory=Default')\n",
    "chrome_options.add_argument('--incognito')\n",
    "chrome_options.add_argument('--disable-plugins-discovery')\n",
    "chrome_options.add_argument('--start-maximized')\n",
    "\n",
    "browser = webdriver.Chrome(path_name, chrome_options=chrome_options)\n",
    "\n",
    "browser.implicitly_wait(8)\n",
    "\n",
    "# The browser.get() method will navigate to a page given by the URL address and wait 15 seconds\n",
    "browser.get('https://www.glassdoor.com/Job/san-mateo-analyst-jobs-SRCH_IL.0,9_IC1147406_KO10,17_IP2.htm?minSalary=120000&jobType=fulltime')\n",
    "sleep(3)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        sleep(2.1)\n",
    "        browser.find_element_by_css_selector('.next').click()\n",
    "\n",
    "        Alert(browser).dismiss()\n",
    "        \n",
    "        #xbutton = browser.find_element_by_class_name('xBtn')\n",
    "        #xbutton = browser.find_element_by_class_name('prettyEmail modalContents')\n",
    "        #xbutton = browser.find_element_by_class_name(' prettyEmail modalContents ')\n",
    "        #xbutton.click()\n",
    "        \n",
    "        #button = browser.find_element_by_xpath(\"//div[contains(@class, 'xBtn')]\")\n",
    "        #button = browser.find_element_by_xpath(\"//div[@class='xBtn']\")\n",
    "        #button = browser.find_element_by_xpath(\"//div[@class=' prettyEmail modalContents ']/div[@class='xBtn']\")\n",
    "        #button.click()\n",
    "        \n",
    "        print('Got rid of the alert!')\n",
    "        sleep(3.3)\n",
    "    except:\n",
    "        print(\"Darn! Couldn't find it :(\")\n",
    "        break\n",
    "\n",
    "#browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create a function `scrape_glassdoor` that scrapes Glassdoor jobs by using Selenium.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_glassdoor(keyword_search, city_search, path_name):\n",
    "    \"\"\"\n",
    "    Function that scrapes Glassdoor jobs and outputs a CSV file of the relevant job posting\n",
    "    information when given the keyword(s), city, and path name of the chrome driver.\n",
    "    \"\"\"\n",
    "    # Specifying incognito mode as you launch your browser\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--profile-directory=Default')\n",
    "    chrome_options.add_argument('--incognito')\n",
    "    chrome_options.add_argument('--disable-plugins-discovery')\n",
    "    chrome_options.add_argument('--start-maximized')\n",
    "\n",
    "    # Create new Instance of Chrome in incognito mode\n",
    "    browser = webdriver.Chrome(path_name, chrome_options=chrome_options)\n",
    "\n",
    "    browser.implicitly_wait(10)\n",
    "\n",
    "    # The browser.get() method will navigate to a page given by the URL address and wait 15 seconds\n",
    "    browser.get('https://www.glassdoor.com/index.htm')\n",
    "\n",
    "    sleep(5)\n",
    "\n",
    "    # Search for bar to enter job title, keywords, or company\n",
    "    job = browser.find_element_by_id('KeywordSearch')\n",
    "\n",
    "    # Search for bar to enter location\n",
    "    location = browser.find_element_by_id('LocationSearch')\n",
    "\n",
    "    # Clear pre-populated location entry\n",
    "    location.clear()\n",
    "\n",
    "    sleep(3)\n",
    "\n",
    "    # Type in job name in search\n",
    "    job.send_keys(keyword_search)\n",
    "\n",
    "    sleep(2)\n",
    "\n",
    "    # Type in location in search\n",
    "    location.send_keys(city_search)\n",
    "\n",
    "    sleep(2)\n",
    "\n",
    "    # Click the search button\n",
    "    browser.find_element_by_class_name('gd-btn-mkt').click()\n",
    "\n",
    "    sleep(8.2)\n",
    "\n",
    "    # Print how many jobs resulted from the search\n",
    "    num_jobs = browser.find_elements_by_xpath(\"//p[@class='jobsCount']\")\n",
    "    sleep(2)\n",
    "    num_jobs = [x.text for x in num_jobs]\n",
    "    num_jobs = num_jobs[0]\n",
    "    num_jobs = int(''.join(c for c in num_jobs if c.isdigit()))\n",
    "    print('There are {0} {1} jobs in {2} on Glassdoor.'.format(num_jobs, keyword_search, city_search))\n",
    "\n",
    "    sleep(3)\n",
    "\n",
    "    # Create an empty list to store our relevant job listings\n",
    "    all_job_listings = []\n",
    "\n",
    "    # Loop through each page and stop when there is no more \"Next\" button\n",
    "    while True:\n",
    "        try:\n",
    "            # Get all job postings on page\n",
    "            job_postings = browser.find_elements_by_xpath('//ul[@class=\"jlGrid hover\"]//descendant::li')\n",
    "            job_post = [x.text for x in job_postings]\n",
    "\n",
    "            sleep(2.8)\n",
    "\n",
    "            # Get all job posting URLs on page\n",
    "            job_urls = browser.find_elements_by_xpath('//div[@class=\"logoWrap\"]/a')\n",
    "            sleep(3.6)\n",
    "            urls = []\n",
    "            for job in job_urls:\n",
    "                url = job.get_attribute('href')\n",
    "                sleep(5.4)\n",
    "                urls.append(url)\n",
    "\n",
    "            sleep(3.2)\n",
    "\n",
    "            # Perform preliminary cleaning to split each part of the string into elements of a list\n",
    "            job_post = [x.encode('utf-8') for x in job_post]\n",
    "            job_post = [x.replace(' \\xe2\\x80\\x93 ', '\\n') for x in job_post]\n",
    "            job_post = [x.splitlines() for x in job_post]\n",
    "            replace_list = ['Hot', 'New', 'Today']\n",
    "            remove_list = ['day ago', 'days ago', ' Logo', 'no.logo.alt', 'Top Company', 'EASY APPLY', \"We're Hiring\"]\n",
    "\n",
    "            sleep(4.1)\n",
    "            \n",
    "            # Standardize the elements within each job post in the order of rating, job title, company, location, and salary\n",
    "            new_job_post = []\n",
    "            for job in job_post:\n",
    "                for i in remove_list:\n",
    "                    for j in replace_list:\n",
    "                        for rating in np.arange(0.0, 5.1, 0.1):\n",
    "                            # Replace specific words with empty string to prevent some jobs from being removed entirely\n",
    "                            job = [x.replace(j, '') for x in job]\n",
    "                            # Remove elements in each job post containing \"days ago\"\n",
    "                            job = [x for x in job if not i in x]\n",
    "                            # Remove all empty elements\n",
    "                            job = [x for x in job if len(x) > 0]\n",
    "                            # Remove ratings\n",
    "                            job = [x for x in job if not x in str(rating)]\n",
    "\n",
    "                # Fill in with empty string if job post is missing a salary\n",
    "                if len(job) == 3:\n",
    "                    job.append('')\n",
    "                elif len(job) == 5:\n",
    "                    job.pop(1)\n",
    "\n",
    "                sleep(3.6)\n",
    "                \n",
    "                # Prints \"Stop\" statement when each job contains less than or more than 4 info (job title, company, location, and salary)\n",
    "                if len(job) != 4:\n",
    "                    print('STOP! Format of job information is off!')\n",
    "                    break\n",
    "\n",
    "                new_job_post.append(job)\n",
    "\n",
    "            sleep(1.8)\n",
    "            \n",
    "            # Add url of each job posting as the last element in each list\n",
    "            for i in range(0, len(urls)):\n",
    "                new_job_post[i].append(urls[i])\n",
    "\n",
    "            sleep(2)\n",
    "            \n",
    "            for job in new_job_post:\n",
    "                all_job_listings.append(job)\n",
    "\n",
    "            browser.find_element_by_css_selector('.next').click()\n",
    "            \n",
    "            sleep(3.3)\n",
    "        except:\n",
    "            print('No more pages!')\n",
    "            break\n",
    "\n",
    "    sleep(4)\n",
    "    \n",
    "    job_listings = []\n",
    "    # Loop through all_job_listings to create a new list job_listings of only the matched jobs\n",
    "    for job in all_job_listings:\n",
    "        # Extract the job title info\n",
    "        jt = job[0]\n",
    "        # Extract the company info\n",
    "        comp = job[1]\n",
    "        # Extract the location info\n",
    "        location = job[2]\n",
    "        # Extract the salary info\n",
    "        salary = job[3]\n",
    "        # Extract the URL info\n",
    "        url = job[4]\n",
    "\n",
    "        # If the job title qualifies our criteria, create a BeautifulSoup string object from the HTML\n",
    "        if(title_qualifies(jt)):    \n",
    "            sleep(2.1)\n",
    "            try:\n",
    "                sleep(4.4)\n",
    "                browser.get(url)\n",
    "                sleep(3.6)\n",
    "                html = browser.page_source\n",
    "                sleep(2.3)\n",
    "                soup = str(BeautifulSoup(html, 'html.parser').encode('utf-8'))\n",
    "                sleep(3)\n",
    "            except:\n",
    "                continue;\n",
    "\n",
    "            # Search through the job posting to sort jobs that fit the level of education and years of experience criteria\n",
    "            a = yr_exper.search(soup)\n",
    "            b = masters1.search(soup)\n",
    "            c = masters2.search(soup)\n",
    "            d = masters3.search(soup)\n",
    "            e = masters4.search(soup)\n",
    "            f = masters5.search(soup)\n",
    "            g = phd1.search(soup)\n",
    "            h = phd2.search(soup)\n",
    "            i = phd3.search(soup)\n",
    "            j = adv_deg.search(soup)\n",
    "\n",
    "            # Append all jobs that fulfill criteria to the job_listings list\n",
    "            if not any([a, b, c, d, e, f, g, h, i, j]):\n",
    "                jobs = {\n",
    "                        'Company': comp,\n",
    "                        'Job Title': jt,\n",
    "                        'Location': location,\n",
    "                        'Salary': salary,\n",
    "                        'URL': url\n",
    "                        }\n",
    "                job_listings.append(jobs)\n",
    "\n",
    "    # Export as a csv file\n",
    "    with open('glassdoor_job_results.csv', 'wb') as csvfile:\n",
    "        fieldnames = ['Company', 'Job Title', 'Location', 'Salary', 'URL']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        if job_listings:\n",
    "            for job in job_listings:\n",
    "                writer.writerow(job)\n",
    "            print('Done!')\n",
    "        else:\n",
    "            print('No matches for Data Science jobs in Glassdoor.')\n",
    "\n",
    "    # Exit out of Chrome        \n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python2.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: use options instead of chrome_options\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5023 data science jobs in San Mateo, CA on Glassdoor.\n",
      "No more pages!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-b68b9ced4b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_glassdoor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data science'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'San Mateo, CA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/Users/ngapuileung/Desktop/DS/chromedriver'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-f56a96ba9033>\u001b[0m in \u001b[0;36mscrape_glassdoor\u001b[0;34m(keyword_search, city_search, path_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# If the job title qualifies our criteria, create a BeautifulSoup string object from the HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_qualifies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape_glassdoor('data science', 'San Mateo, CA', '/Users/ngapuileung/Desktop/DS/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove duplicates in the csv file if applicable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "glassdoor_search_results = pd.read_csv('glassdoor_job_results.csv', header=0)\n",
    "glassdoor_search_results = glassdoor_search_results.drop_duplicates()\n",
    "glassdoor_search_results.to_csv('glassdoor_job_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
